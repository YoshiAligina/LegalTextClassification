{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  case_id case_outcome                                         case_title  \\\n",
      "0   Case1        cited  Alpine Hardwood (Aust) Pty Ltd v Hardys Pty Lt...   \n",
      "1   Case2        cited  Black v Lipovac [1998] FCA 699 ; (1998) 217 AL...   \n",
      "2   Case3        cited  Colgate Palmolive Co v Cussons Pty Ltd (1993) ...   \n",
      "3   Case4        cited  Dais Studio Pty Ltd v Bullett Creative Pty Ltd...   \n",
      "4   Case5        cited  Dr Martens Australia Pty Ltd v Figgins Holding...   \n",
      "\n",
      "                                           case_text  \n",
      "0  Ordinarily that discretion will be exercised s...  \n",
      "1  The general principles governing the exercise ...  \n",
      "2  Ordinarily that discretion will be exercised s...  \n",
      "3  The general principles governing the exercise ...  \n",
      "4  The preceding general principles inform the ex...  \n",
      "Columns in the dataset: Index(['case_id', 'case_outcome', 'case_title', 'case_text'], dtype='object')\n",
      "case_id           0\n",
      "case_outcome      0\n",
      "case_title        0\n",
      "case_text       176\n",
      "dtype: int64\n",
      "case_outcome\n",
      "cited            12219\n",
      "referred to       4384\n",
      "applied           2448\n",
      "followed          2256\n",
      "considered        1712\n",
      "discussed         1024\n",
      "distinguished      608\n",
      "related            113\n",
      "affirmed           113\n",
      "approved           108\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\yoshi\\OneDrive\\Desktop\\CSMaster\\CS439\\FInalProj\\legalData\\legal_text_classification.csv\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Print the column names to verify\n",
    "print(\"Columns in the dataset:\", df.columns)\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check the distribution of case outcomes\n",
    "if 'case_outcome' in df.columns:\n",
    "    print(df['case_outcome'].value_counts())\n",
    "else:\n",
    "    print(\"The 'case_outcome' column is not present in the dataset.\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           case_text  \\\n",
      "0  Ordinarily that discretion will be exercised s...   \n",
      "1  The general principles governing the exercise ...   \n",
      "2  Ordinarily that discretion will be exercised s...   \n",
      "3  The general principles governing the exercise ...   \n",
      "4  The preceding general principles inform the ex...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  ordinarily discretion exercised cost follow ev...  \n",
      "1  general principle governing exercise discretio...  \n",
      "2  ordinarily discretion exercised cost follow ev...  \n",
      "3  general principle governing exercise discretio...  \n",
      "4  preceding general principle inform exercise di...  \n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Tokenize text\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        # Join tokens back into a string\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Apply preprocessing to the case_text column\n",
    "df['cleaned_text'] = df['case_text'].apply(preprocess_text)\n",
    "\n",
    "# Display the cleaned text\n",
    "print(df[['case_text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (19988,) (19988,)\n",
      "Testing set shape: (4997,) (4997,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['cleaned_text'], df['case_outcome'], test_size=0.2, random_state=777\n",
    ")\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5555333199919952\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     affirmed       0.60      0.33      0.43        18\n",
      "      applied       0.35      0.18      0.24       488\n",
      "     approved       0.33      0.05      0.08        22\n",
      "        cited       0.61      0.84      0.71      2471\n",
      "   considered       0.36      0.17      0.23       335\n",
      "    discussed       0.41      0.12      0.19       237\n",
      "distinguished       0.55      0.16      0.25       112\n",
      "     followed       0.42      0.27      0.33       430\n",
      "  referred to       0.48      0.44      0.46       857\n",
      "      related       1.00      0.15      0.26        27\n",
      "\n",
      "     accuracy                           0.56      4997\n",
      "    macro avg       0.51      0.27      0.32      4997\n",
      " weighted avg       0.52      0.56      0.51      4997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with scaling and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MaxAbsScaler()),\n",
    "    ('logistic', LogisticRegression(max_iter=500, solver='liblinear', C=1.0, penalty='l2'))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the pipeline (model) and vectorizer\n",
    "joblib.dump(pipeline, 'legal_text_classification_model.pkl')\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: referred to\n"
     ]
    }
   ],
   "source": [
    "# Load the model and vectorizer\n",
    "model = joblib.load('legal_text_classification_model.pkl')\n",
    "tfidf = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "# Preprocess new text\n",
    "new_text = \"This is a new legal case about intellectual property.\"\n",
    "cleaned_text = preprocess_text(new_text)\n",
    "\n",
    "# Transform the text using TF-IDF\n",
    "new_text_tfidf = tfidf.transform([cleaned_text])\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict(new_text_tfidf)\n",
    "print(\"Predicted Label:\", prediction[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
