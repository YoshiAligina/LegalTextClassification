{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0293f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca83f5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded:\n",
      "  case_id case_outcome                                         case_title  \\\n",
      "0   Case1        cited  Alpine Hardwood (Aust) Pty Ltd v Hardys Pty Lt...   \n",
      "1   Case2        cited  Black v Lipovac [1998] FCA 699 ; (1998) 217 AL...   \n",
      "2   Case3        cited  Colgate Palmolive Co v Cussons Pty Ltd (1993) ...   \n",
      "3   Case4        cited  Dais Studio Pty Ltd v Bullett Creative Pty Ltd...   \n",
      "4   Case5        cited  Dr Martens Australia Pty Ltd v Figgins Holding...   \n",
      "\n",
      "                                           case_text  \n",
      "0  Ordinarily that discretion will be exercised s...  \n",
      "1  The general principles governing the exercise ...  \n",
      "2  Ordinarily that discretion will be exercised s...  \n",
      "3  The general principles governing the exercise ...  \n",
      "4  The preceding general principles inform the ex...  \n",
      "Columns: Index(['case_id', 'case_outcome', 'case_title', 'case_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# --- Load Dataset ---\n",
    "file_path = r\"C:\\Users\\yoshi\\OneDrive\\Desktop\\CSMaster\\CS439\\FInalProj\\legalData\\legal_text_classification.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display dataset info\n",
    "print(\"Dataset loaded:\")\n",
    "print(df.head())\n",
    "print(\"Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8090e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed text:\n",
      "                                           case_text  \\\n",
      "0  Ordinarily that discretion will be exercised s...   \n",
      "1  The general principles governing the exercise ...   \n",
      "2  Ordinarily that discretion will be exercised s...   \n",
      "3  The general principles governing the exercise ...   \n",
      "4  The preceding general principles inform the ex...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  ordinarily discretion exercised cost follow ev...  \n",
      "1  general principle governing exercise discretio...  \n",
      "2  ordinarily discretion exercised cost follow ev...  \n",
      "3  general principle governing exercise discretio...  \n",
      "4  preceding general principle inform exercise di...  \n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    return ''\n",
    "\n",
    "df['cleaned_text'] = df['case_text'].apply(preprocess_text)\n",
    "print(\"Preprocessed text:\")\n",
    "print(df[['case_text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea9b982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned categories:\n",
      "                                        cleaned_text case_category\n",
      "0  ordinarily discretion exercised cost follow ev...         other\n",
      "1  general principle governing exercise discretio...         other\n",
      "2  ordinarily discretion exercised cost follow ev...         other\n",
      "3  general principle governing exercise discretio...         other\n",
      "4  preceding general principle inform exercise di...         other\n"
     ]
    }
   ],
   "source": [
    "keywords = {\n",
    "    'family': ['children', 'custody', 'divorce', 'marriage', 'adoption', 'parenting orders', 'child support', 'spousal maintenance', 'family violence', 'guardianship', 'prenuptial agreements'],\n",
    "    'property': ['property', 'ownership', 'land', 'real estate', 'lease', 'easements', 'mortgages', 'foreclosure', 'zoning', 'landlord', 'tenant', 'eviction'],\n",
    "    'criminal': ['theft', 'murder', 'assault', 'fraud', 'crime', 'sentencing', 'bail', 'parole', 'prosecution', 'homicide', 'robbery', 'drug offenses'],\n",
    "    'business': ['contract', 'agreement', 'corporation', 'partnership', 'mergers', 'franchises', 'intellectual property', 'trade practices'],\n",
    "    'financial_and_securities': ['securities', 'investments', 'insider trading', 'market manipulation', 'financial services'],\n",
    "    'administrative': ['judicial review', 'government decisions', 'statutory interpretation'],\n",
    "    'employment': ['workers comp']\n",
    "}\n",
    "\n",
    "def assign_category(text):\n",
    "    for category, words in keywords.items():\n",
    "        if any(word in text.lower() for word in words):\n",
    "            return category\n",
    "    return 'other'\n",
    "\n",
    "df['case_category'] = df['cleaned_text'].apply(assign_category)\n",
    "print(\"Assigned categories:\")\n",
    "print(df[['cleaned_text', 'case_category']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bb921d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text saved to legalData\\cleaned_legal_text.csv\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'legalData'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, 'cleaned_legal_text.csv')\n",
    "df[['case_text', 'cleaned_text']].to_csv(output_file, index=False)\n",
    "print(f\"Cleaned text saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5193496a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yoshi\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered dataset:\n",
      "                                        cleaned_text  cluster case_category\n",
      "0  ordinarily discretion exercised cost follow ev...        1        family\n",
      "1  general principle governing exercise discretio...        2        family\n",
      "2  ordinarily discretion exercised cost follow ev...        1        family\n",
      "3  general principle governing exercise discretio...        2        family\n",
      "4  preceding general principle inform exercise di...        1        family\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(df['cleaned_text'])\n",
    "\n",
    "num_clusters = len(keywords)\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=777)\n",
    "df['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# Map clusters to categories\n",
    "cluster_to_category = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_docs = df[df['cluster'] == cluster]['cleaned_text']\n",
    "    cluster_keywords = ' '.join(cluster_docs).split()\n",
    "    for category, words in keywords.items():\n",
    "        if any(word in cluster_keywords for word in words):\n",
    "            cluster_to_category[cluster] = category\n",
    "            break\n",
    "    else:\n",
    "        cluster_to_category[cluster] = 'other'\n",
    "\n",
    "df['case_category'] = df['cluster'].map(cluster_to_category)\n",
    "print(\"Clustered dataset:\")\n",
    "print(df[['cleaned_text', 'cluster', 'case_category']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d6e48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered dataset saved to legalData\\clustered_legal_text.csv\n"
     ]
    }
   ],
   "source": [
    "clustered_file = os.path.join(output_dir, 'clustered_legal_text.csv')\n",
    "df.to_csv(clustered_file, index=False)\n",
    "print(f\"Clustered dataset saved to {clustered_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0423e373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.01\n",
      "Calinski-Harabasz Index: 192.60\n"
     ]
    }
   ],
   "source": [
    "silhouette_avg = silhouette_score(tfidf_matrix, df['cluster'])\n",
    "calinski_harabasz = calinski_harabasz_score(tfidf_matrix.toarray(), df['cluster'])\n",
    "print(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7899a1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 'family'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m\n\u001b[0;32m     10\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     11\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m'\u001b[39m, TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)),\n\u001b[0;32m     12\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, MaxAbsScaler()),\n\u001b[0;32m     13\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic\u001b[39m\u001b[38;5;124m'\u001b[39m, LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     14\u001b[0m ])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[0;32m     20\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\yoshi\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    404\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\yoshi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1216\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1211\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1212\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m > 1 does not have any effect when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1213\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs))\n\u001b[0;32m   1215\u001b[0m         )\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m _fit_liblinear(\n\u001b[0;32m   1217\u001b[0m         X,\n\u001b[0;32m   1218\u001b[0m         y,\n\u001b[0;32m   1219\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC,\n\u001b[0;32m   1220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[0;32m   1221\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_scaling,\n\u001b[0;32m   1222\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m   1223\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty,\n\u001b[0;32m   1224\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual,\n\u001b[0;32m   1225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   1226\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[0;32m   1227\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m   1228\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[0;32m   1229\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1230\u001b[0m     )\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\yoshi\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1181\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m   1179\u001b[0m     classes_ \u001b[38;5;241m=\u001b[39m enc\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(classes_) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1181\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1182\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1183\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1184\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1185\u001b[0m             \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1186\u001b[0m         )\n\u001b[0;32m   1188\u001b[0m     class_weight_ \u001b[38;5;241m=\u001b[39m compute_class_weight(class_weight, classes\u001b[38;5;241m=\u001b[39mclasses_, y\u001b[38;5;241m=\u001b[39my)\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 'family'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['cleaned_text'], \n",
    "    df['case_category'], \n",
    "    test_size=0.2, \n",
    "    random_state=777, \n",
    "    stratify=df['case_category']\n",
    ")\n",
    "\n",
    "# --- Model Pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('scaler', MaxAbsScaler()),\n",
    "    ('logistic', LogisticRegression(max_iter=500, solver='liblinear', C=1.0, penalty='l2'))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"Classification Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7132c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'legal_text_classification_model.pkl'\n",
    "joblib.dump(pipeline, model_file)\n",
    "print(f\"Model saved to {model_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
