{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# --- Load Dataset ---\n",
    "file_path = r\"C:\\Users\\yoshi\\OneDrive\\Desktop\\CSMaster\\CS439\\FInalProj\\legalData\\legal_text_classification.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display dataset info\n",
    "print(\"Dataset loaded:\")\n",
    "print(df.head())\n",
    "print(\"Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8090e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    return ''\n",
    "\n",
    "df['cleaned_text'] = df['case_text'].apply(preprocess_text)\n",
    "print(\"Preprocessed text:\")\n",
    "print(df[['case_text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea9b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\n",
    "    'family': ['children', 'custody', 'divorce', 'marriage', 'adoption', 'parenting orders', 'child support', 'spousal maintenance', 'family violence', 'guardianship', 'prenuptial agreements'],\n",
    "    'property': ['property', 'ownership', 'land', 'real estate', 'lease', 'easements', 'mortgages', 'foreclosure', 'zoning', 'landlord', 'tenant', 'eviction'],\n",
    "    'criminal': ['theft', 'murder', 'assault', 'fraud', 'crime', 'sentencing', 'bail', 'parole', 'prosecution', 'homicide', 'robbery', 'drug offenses'],\n",
    "    'business': ['contract', 'agreement', 'corporation', 'partnership', 'mergers', 'franchises', 'intellectual property', 'trade practices'],\n",
    "    'financial_and_securities': ['securities', 'investments', 'insider trading', 'market manipulation', 'financial services'],\n",
    "    'administrative': ['judicial review', 'government decisions', 'statutory interpretation'],\n",
    "    'employment': ['workers comp']\n",
    "}\n",
    "\n",
    "def assign_category(text):\n",
    "    for category, words in keywords.items():\n",
    "        if any(word in text.lower() for word in words):\n",
    "            return category\n",
    "    return 'other'\n",
    "\n",
    "df['case_category'] = df['cleaned_text'].apply(assign_category)\n",
    "print(\"Assigned categories:\")\n",
    "print(df[['cleaned_text', 'case_category']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb921d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'legalData'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, 'cleaned_legal_text.csv')\n",
    "df[['case_text', 'cleaned_text']].to_csv(output_file, index=False)\n",
    "print(f\"Cleaned text saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(df['cleaned_text'])\n",
    "\n",
    "num_clusters = len(keywords)\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=777)\n",
    "df['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# Map clusters to categories\n",
    "cluster_to_category = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_docs = df[df['cluster'] == cluster]['cleaned_text']\n",
    "    cluster_keywords = ' '.join(cluster_docs).split()\n",
    "    for category, words in keywords.items():\n",
    "        if any(word in cluster_keywords for word in words):\n",
    "            cluster_to_category[cluster] = category\n",
    "            break\n",
    "    else:\n",
    "        cluster_to_category[cluster] = 'other'\n",
    "\n",
    "df['case_category'] = df['cluster'].map(cluster_to_category)\n",
    "print(\"Clustered dataset:\")\n",
    "print(df[['cleaned_text', 'cluster', 'case_category']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d6e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_file = os.path.join(output_dir, 'clustered_legal_text.csv')\n",
    "df.to_csv(clustered_file, index=False)\n",
    "print(f\"Clustered dataset saved to {clustered_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg = silhouette_score(tfidf_matrix, df['cluster'])\n",
    "calinski_harabasz = calinski_harabasz_score(tfidf_matrix.toarray(), df['cluster'])\n",
    "print(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7899a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['case_category'], test_size=0.2, random_state=777)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('scaler', MaxAbsScaler()),\n",
    "    ('logistic', LogisticRegression(max_iter=500, solver='liblinear', C=1.0, penalty='l2'))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7132c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'legal_text_classification_model.pkl'\n",
    "joblib.dump(pipeline, model_file)\n",
    "print(f\"Model saved to {model_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
